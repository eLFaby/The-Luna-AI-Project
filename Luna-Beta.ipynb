{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports + checking if CUDA is enabled first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is enabled \n",
    "import torch\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import tkinter as tk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "import json\n",
    "torch.cuda.empty_cache()   # Emptying Cuda cache in order to free important space\n",
    "os.environ[\"KERAS_BACKEND\"]=\"torch\" # options 'torch' / 'jax' / 'tensorflow'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='1.00'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for fine-tuning<br>\n",
    "Dataset type: Instruction - Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/devy/.var/app/com.visualstudio.code/cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "data=[]\n",
    "datasetlocation='./model/dataset/databricks-dolly-15k.jsonl'    # Change accordingly, might want to change function below as well\n",
    "with open(datasetlocation) as file:\n",
    "    for line in file:\n",
    "        features=json.loads(line)\n",
    "        if features['context']:    # Uncomment to skip 'context' column\n",
    "          continue\n",
    "        template=\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\" \n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# Alternative to above\n",
    "#data=load_dataset('databricks/databricks-dolly-15k')\n",
    "########################################################\n",
    "\n",
    "# Need permission from Gemma first (form completion)\n",
    "# Loading Huggingface Token and using login function\n",
    "\n",
    "with open(\"token.json\", \"r\") as json_file:\n",
    "    token_dict = json.load(json_file)\n",
    "\n",
    "access_token = token_dict[\"token\"]\n",
    "from huggingface_hub import login\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Google's Gemma 2B from HuggingFace<br> alternatives: 'google/gemma-2b-it', 'google/gemma-7b', 'google/gemma-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "core='google/gemma-2b'   # Model name, theoretically code should work for a different model as long as it is not too different from gemma-2b\n",
    "cache_dir='./model/gemma'# Cache location where the model will be stored\n",
    "#\n",
    "\n",
    "# Gemma chat/prompt template: \n",
    "# <bos><start_of_turn>user\n",
    "# Message <end_of_turn>\n",
    "# <start_of_turn>model\n",
    "\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments=True\n",
    "\n",
    "# BitsAndBytesConfig 4bit or 8bit\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type='nf4', \n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "# Model to be used \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    core, # model name\n",
    "    cache_dir=cache_dir, # location to be saved at\n",
    "    device_map='auto', \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "# Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(core,cache_dir=cache_dir)\n",
    "tokenizer.padding_side='right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devy/miniconda3/envs/AI/lib/python3.9/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/devy/miniconda3/envs/AI/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "def send_message(event=None):\n",
    "    message = entry.get()\n",
    "    if message.lower() == 'exit': # keyword to exit window\n",
    "        root.quit()\n",
    "        return\n",
    "    # Generate response\n",
    "    inputs = tokenizer(message, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=50, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Display message and response\n",
    "    chat_area.insert(tk.END, f\"You: {message}\\n\")\n",
    "    chat_area.insert(tk.END, f\"Bot: {response}\\n\\n\")\n",
    "    entry.delete(0, tk.END)\n",
    "\n",
    "# Create GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "chat_area = tk.Text(root, width=50, height=20)\n",
    "scrollbar = tk.Scrollbar(root, command=chat_area.yview)\n",
    "chat_area.configure(yscrollcommand=scrollbar.set)\n",
    "entry = tk.Entry(root, width=50)\n",
    "entry.bind(\"<Return>\", send_message)\n",
    "\n",
    "chat_area.pack(side=tk.TOP, fill=tk.BOTH, padx=5, pady=5)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "entry.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)\n",
    "\n",
    "entry.focus_set()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning<br>\n",
    "Option 1: qLoRA - Low rank adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha= 32,\n",
    "    lora_dropout=0.05,\n",
    "    # bias=\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # linear layers\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model=get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 78446592 | Total parameters: 2584619008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "# Total and trainable parameters check\n",
    "trainable, total =model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable parameters: {trainable} | Total parameters: {total} | Percentage: {trainable/total*100:.4f}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be modified if model found lacking, generally already optimized parameters\n",
    "args = TrainingArguments(\n",
    "    output_dir='Gemma-2B-Dolly-FT',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=2,\n",
    "    optim='adamw_torch_fused',\n",
    "    logging_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    bf16=False, # current gpu does not support this param / enable if supported\n",
    "    tf32=False, # current gpu does not support this param either / enable if supported\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,                                     # Based on QLoRA paper\n",
    "    max_grad_norm=0.3,                                      # Based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                      # Based on QLoRA paper\n",
    "    lr_scheduler_type='constant',\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
