{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 14:11:26.601095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 14:11:27.795288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-24 14:11:29.922229: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 14:11:29.922911: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hide warnings\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "# Limiting GPU growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()   # Emptying Cuda cache in order to free important space\n",
    "torch.cuda.is_available()\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session()   # Same but for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devy/miniconda3/envs/AI/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.36s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "# path to model ./model/Llama3/models--nvidia--Llama3-ChatQA-1.5-8B/snapshots/2a579cf6db7bbf49b138d4026dae6c8f822fc3de/\n",
    "\n",
    "\n",
    "model_path= \"./model/Llama3/models--nvidia--Llama3-ChatQA-1.5-8B/snapshots/2a579cf6db7bbf49b138d4026dae6c8f822fc3de/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.float16,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size and context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is: 128256\n",
      "The context length is: 8192 tokens\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer)\n",
    "print(f\"The vocabulary size is: {vocab_size}\")\n",
    "context_length = model.config.max_position_embeddings\n",
    "print(f\"The context length is: {context_length} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give a full and complete answer for the question\n",
      "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function that allows user to modify the AI's default instruction\n",
    "def set_instruction(custom_instruction=None):\n",
    "    default_instruction = 'Please give a full and complete answer for the question'\n",
    "    if custom_instruction:\n",
    "        return custom_instruction\n",
    "    else:\n",
    "        return default_instruction\n",
    "\n",
    "print (set_instruction())\n",
    "\n",
    "# Function that allows user to modify the system context\n",
    "def set_system(custom_system=None):\n",
    "    default_system=\"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "    if custom_system:\n",
    "        return custom_system\n",
    "    else:\n",
    "        return default_system\n",
    "\n",
    "print(set_system())\n",
    "\n",
    "# Function that reads, if necesessary, a document and prints it for the AI to read, the default code is 'document=\"\"\" This is a document \"\"\"'\n",
    "# The document is called via formatted_input = get_formatted_input(messages, document)\n",
    "def set_document(custom_doc=None):\n",
    "    if custom_doc:\n",
    "        with open(custom_doc, 'r') as f:\n",
    "            document=f.read()\n",
    "        return document\n",
    "    else:\n",
    "        return \"\"\n",
    "print (set_document())\n",
    "\n",
    "# Function that handles user messages.\n",
    "def create_message(message):\n",
    "    messages =[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hello darkness my old friend'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_message(\"Hello darkness my old friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code. Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello there! How can I help you?\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "message=\"Hello there\"\n",
    "\n",
    "\n",
    "# Message = user input\n",
    "messages = create_message(message)\n",
    "\n",
    "# Set/use doc if needed\n",
    "document = set_document()\n",
    "def get_formatted_input(messages, context):\n",
    "    # setting system context and AI instruction \n",
    "    system = set_system()\n",
    "    instruction=set_instruction()\n",
    "\n",
    "    for item in messages:\n",
    "        if item['role'] == \"user\":\n",
    "            ## only apply this instruction for the first user turn\n",
    "            item['content'] = instruction + \" \" + item['content']\n",
    "            break\n",
    "\n",
    "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "formatted_input = get_formatted_input(messages, document)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n",
    "\n",
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
