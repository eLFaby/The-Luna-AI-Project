{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if CUDA is enabled first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is enabled \n",
    "import torch\n",
    "import os\n",
    "import tensorflow as tf\n",
    "torch.cuda.empty_cache()   # Emptying Cuda cache in order to free important space\n",
    "os.environ[\"KERAS_BACKEND\"]=\"tensorflow\" # options 'torch' / 'jax' / 'tensorflow'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for fine-tuning<br>\n",
    "Dataset type: Instruction - Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/devy/.var/app/com.visualstudio.code/cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "import json\n",
    "data=[]\n",
    "datasetlocation='./model/dataset/databricks-dolly-15k.jsonl'    # Change accordingly, might want to change function below as well\n",
    "with open(datasetlocation) as file:\n",
    "    for line in file:\n",
    "        features=json.loads(line)\n",
    "        # if features['context']:    # Uncomment to skip 'context' column for simplicity\n",
    "        #    continue\n",
    "        template=\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\" \n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# Alternative to above\n",
    "# data='databricks/databricks-dolly-15k' or something else\n",
    "# from datasets import load_dataset\n",
    "# dataset=load_dataset(data)\n",
    "########################################################\n",
    "\n",
    "# Need permission from Gemma first (form completion)\n",
    "# Loading Huggingface Token and using login function\n",
    "\n",
    "with open(\"token.json\", \"r\") as json_file:\n",
    "    token_dict = json.load(json_file)\n",
    "\n",
    "access_token = token_dict[\"token\"]\n",
    "from huggingface_hub import login\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Google's Gemma 2B from HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# \n",
    "core='google/gemma-2b'   # Model name, theoretically code should work for a different model as long as it is not too different from gemma-2b\n",
    "cache_dir='./model/gemma'# Cache location where the model will be stored\n",
    "#\n",
    "\n",
    "# Gemma chat/prompt template: \n",
    "# <bos><start_of_turn>user\n",
    "# Message <end_of_turn>\n",
    "# <start_of_turn>model\n",
    "\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments=True\n",
    "\n",
    "# BitsAndBytesConfig 4bit, alternative: 8-bit\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4', \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "# Model to be used\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    core,\n",
    "    cache_dir=cache_dir,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "# Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(core,cache_dir=cache_dir)\n",
    "tokenizer.padding_side='right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "def send_message(event=None):\n",
    "    message = entry.get()\n",
    "    if message.lower() == 'exit':\n",
    "        root.quit()\n",
    "        return\n",
    "    # Generate response\n",
    "    inputs = tokenizer(message, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=500, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Display message and response\n",
    "    chat_area.insert(tk.END, f\"You: {message}\\n\")\n",
    "    chat_area.insert(tk.END, f\"Bot: {response}\\n\\n\")\n",
    "    entry.delete(0, tk.END)\n",
    "\n",
    "# Create GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "chat_area = tk.Text(root, width=50, height=20)\n",
    "scrollbar = tk.Scrollbar(root, command=chat_area.yview)\n",
    "chat_area.configure(yscrollcommand=scrollbar.set)\n",
    "entry = tk.Entry(root, width=50)\n",
    "entry.bind(\"<Return>\", send_message)\n",
    "\n",
    "chat_area.pack(side=tk.TOP, fill=tk.BOTH, padx=5, pady=5)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "entry.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)\n",
    "\n",
    "entry.focus_set()\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning<br>\n",
    "Option 1: LoRA - Low rank adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha= 32,\n",
    "    lora_dropout=0.05,\n",
    "    # bias=\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # linear layers\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model=get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 78446592 | Total parameters: 2584619008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "trainable, total =model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable parameters: {trainable} | Total parameters: {total} | Percentage: {trainable/total*100:.4f}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='Gemma-2B-Dolly-FT',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=2,\n",
    "    optim='adamw_torch_fused',\n",
    "    logging_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    bf16=False, # current gpu does not support this\n",
    "    tf32=False, # current gpu does not support this either\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,                                     # Based on QLoRA paper\n",
    "    max_grad_norm=0.3,                                      # Based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                      # Based on QLoRA paper\n",
    "    lr_scheduler_type='constant',\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2352635195.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[230], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    peft_config=lora_config,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example)):\n",
    "        text = f\"Instruction: {example['Instruction'][i]}\\nResponse: {example['response'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\"\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\":False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# .. and then saving it\n",
    "trainer.save_model(core+'/Gemma2bFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "input = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input)\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "prompt=template.format(\n",
    "    instruction=\"Can you introduce yourself?\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(model.generate(prompt,max_length=256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
