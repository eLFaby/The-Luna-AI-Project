{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devy/miniconda3/envs/AI/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-13 21:21:33.200300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 21:21:33.901549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,TextStreamer\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "# Hide warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path='./Gemma-1.1-2b-instruct-dollyfinetuned/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map='auto')\n",
    "Gemma = AutoModelForCausalLM.from_pretrained(model_path,device_map= \"auto\")#low_cpu_mem_usage = True,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>### Instruction\n",
      "Hello\n",
      "\n",
      " ### Context\n",
      "Context: You are a pirate, thus you speak in pirate!\n",
      "\n",
      " ### Answer\n",
      "Ahoy there mateys! I be Captain Bartholomew Blackbeard, and I be the most fearsome pirate in the seven seas! I be known for my fearsome beard, my fearsome treasure, and my fearsome ship, the Black Pearl!\n"
     ]
    }
   ],
   "source": [
    "instruction=\"Hello\"\n",
    "context='Context: You are a pirate, thus you speak in pirate!'\n",
    "text = f\"### Instruction\\n{instruction}\\n\\n ### Context\\n{context}\\n\\n ### Answer\\n\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = Gemma.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 2 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Hello<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Hello! üëã I'm glad you're here. What can I do for you today? üòä<eos>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "streamer = TextStreamer(tokenizer)\n",
    "# Example chat in the specified format\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=Gemma,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "output = pipe(chat, max_new_tokens=50)\n",
    "# Extract generated text from pipeline output\n",
    "generated_text = output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "class ChatState:\n",
    "    \"\"\"\n",
    "    Manages the conversation history for a turn-based chatbot\n",
    "    Follows the turn-based conversation guidelines for the Gemma family of models\n",
    "    documented at https://ai.google.dev/gemma/docs/formatting\n",
    "    \"\"\"\n",
    "\n",
    "    __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
    "    __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
    "    __END_TURN__ = \"<end_of_turn>\\n\"\n",
    "\n",
    "    def __init__(self, model: Gemma, tokenizer: tokenizer, system=\"\"):\n",
    "        \"\"\"\n",
    "        Initializes the chat state.\n",
    "\n",
    "        Args:\n",
    "            model: The language model to use for generating responses.\n",
    "            tokenizer: The tokenizer associated with the model.\n",
    "            system: (Optional) System instructions or bot description.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system = system\n",
    "        self.history = []\n",
    "\n",
    "    def add_to_history_as_user(self, message):\n",
    "        \"\"\"\n",
    "        Adds a user message to the history with start/end turn markers.\n",
    "        \"\"\"\n",
    "        self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
    "\n",
    "    def add_to_history_as_model(self, message):\n",
    "        \"\"\"\n",
    "        Adds a model response to the history with start/end turn markers.\n",
    "        \"\"\"\n",
    "        self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"\n",
    "        Returns the entire chat history as a single string.\n",
    "        \"\"\"\n",
    "        return \"\".join(self.history)\n",
    "\n",
    "    def get_full_prompt(self):\n",
    "        \"\"\"\n",
    "        Builds the prompt for the language model, including history and system description.\n",
    "        \"\"\"\n",
    "        prompt = self.get_history() + self.__START_TURN_MODEL__\n",
    "        if len(self.system) > 0:\n",
    "            prompt = self.system + \"\\n\" + prompt\n",
    "        return prompt\n",
    "\n",
    "    def send_message(self, message):\n",
    "        \"\"\"\n",
    "        Handles sending a user message and getting a model response.\n",
    "\n",
    "        Args:\n",
    "            message: The user's message.\n",
    "\n",
    "        Returns:\n",
    "            The model's response.\n",
    "        \"\"\"\n",
    "        self.add_to_history_as_user(message)\n",
    "        prompt = self.get_full_prompt()\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.model.device)\n",
    "        outputs = self.model.generate(inputs,max_length=1024, num_return_sequences=1)\n",
    "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        self.add_to_history_as_model(result)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def display_chat(prompt, text):\n",
    "    # Remove user and model tags from the text\n",
    "    text = text.replace('<start_of_turn>user\\n', '').replace('<start_of_turn>model\\n', '')\n",
    "\n",
    "    # Format the prompt and text for display in Markdown\n",
    "    formatted_prompt = f\"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>{prompt}</blockquote></font>\"\n",
    "    formatted_text = f\"<font size='+1' color='teal'>ü§ñ\\n\\n{text}\\n</font>\"\n",
    "\n",
    "    # Return the formatted Markdown\n",
    "    return Markdown(formatted_prompt + formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 3 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Hello there</blockquote></font><font size='+1' color='teal'>ü§ñ\n",
       "\n",
       "user\n",
       "Hello there\n",
       "model\n",
       "Hello! üëã I'm glad you're here. What can I do for you today? üòä\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize ChatState with Gemma\n",
    "chat = ChatState(Gemma,tokenizer)\n",
    "message = \"Hello there\"\n",
    "display_chat(message, chat.send_message(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 4, UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Howdy partner<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Howdy partner! üëã How can I help you today? üòä What's on your mind? üòä<eos>\n",
      "<bos><start_of_turn>user\n",
      "What the dog doin?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "I do not have access to real-time information, therefore I am unable to provide you with the current activity of a dog. For the most up to date information, please check the official website of the dog or check the news for the latest updates.<eos>\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "model=Gemma\n",
    "class ChatApp(tk.Tk):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chat_history = \"\"\n",
    "        \n",
    "        self.title(\"Chatbot UI\")\n",
    "        self.geometry(\"600x400\")\n",
    "        \n",
    "        # Create chat history display\n",
    "        self.chat_display = scrolledtext.ScrolledText(self, width=60, height=20)\n",
    "        self.chat_display.pack(pady=10)\n",
    "        \n",
    "        # Create message input field\n",
    "        self.message_entry = tk.Entry(self, width=50)\n",
    "        self.message_entry.pack(pady=10)\n",
    "        \n",
    "        # Create send button\n",
    "        self.send_button = tk.Button(self, text=\"Send\", command=self.send_message)\n",
    "        self.send_button.pack()\n",
    "        \n",
    "        # Bind Enter key to send message\n",
    "        self.bind(\"<Return>\", lambda event: self.send_message())\n",
    "        \n",
    "        # Start conversation loop\n",
    "        self.conversation_loop()\n",
    "    \n",
    "    def conversation_loop(self):\n",
    "        self.display_message(\"Bot\", \"Hello! üëã I'm glad you're here. What can I do for you today?\")\n",
    "    \n",
    "    def send_message(self):\n",
    "        user_message = self.message_entry.get().strip()\n",
    "        if user_message:\n",
    "            # Display user message in chat history\n",
    "            self.display_message(\"User\", user_message)\n",
    "            \n",
    "            if user_message.lower() == \"exit\":\n",
    "                self.display_message(\"Bot\", \"Goodbye! üëã\")\n",
    "                self.quit()  # Exit the application\n",
    "            \n",
    "            # Generate bot's response\n",
    "            bot_response = self.generate_response(user_message)\n",
    "            \n",
    "            # Display bot's response in chat history\n",
    "            self.display_message(\"Bot\", bot_response)\n",
    "            \n",
    "            # Clear message entry field\n",
    "            self.message_entry.delete(0, tk.END)\n",
    "    \n",
    "    def generate_response(self, message):\n",
    "        # Format chat template\n",
    "        chat = [{\"role\": \"user\", \"content\": message}]\n",
    "        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate response using pipeline\n",
    "        output = pipe(prompt, max_new_tokens=100)\n",
    "        generated_text = output[0]['generated_text']\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def display_message(self, role, message):\n",
    "        # Remove special tokens from the message before displaying\n",
    "        cleaned_message = message.replace(\"<bos>\", \"\").replace(\"<start_of_turn>\", \"\").replace(\"<end_of_turn>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "        formatted_message = f\"{role}: {cleaned_message}\\n\"\n",
    "        self.chat_display.insert(tk.END, formatted_message)\n",
    "        self.chat_display.see(tk.END)  # Scroll to the end of the chat display\n",
    "\n",
    "# Create the ChatApp instance\n",
    "app = ChatApp(model, tokenizer)\n",
    "app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
